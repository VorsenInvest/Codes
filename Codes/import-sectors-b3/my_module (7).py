# -*- coding: utf-8 -*-
"""my_module

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rIPidaH-rq0BmZS2KeUgtRrrDd1IFI_X
"""

import os
import zipfile
import pandas as pd
import requests
import ipywidgets as widgets
from IPython.display import display
import numpy as np


def import_API_data():
    # API URL
    url = "https://brapi.dev/api/quote/list?token=fUNoicPSZTzstUdqw1uVeB"

    # Send a GET request to the API
    response = requests.get(url)

    # Check if the request was successful
    if response.status_code == 200:
        data = response.json()
    else:
        print("Failed to retrieve data:", response.status_code)
        return None  # Return None in case of failure

    # Access the 'stocks' part of the data
    stocks_data = data['stocks']

    # Filter to keep only components where 'type' is 'stock' and not ending with 'F'
    filtered_stocks = [item for item in stocks_data if item.get('type') == 'stock' and not item.get('stock', '').endswith('F')]

    # Create a DataFrame from the filtered stock data
    stock_table = pd.DataFrame(filtered_stocks)

    # Initialize empty lists
    priceEarnings_list, trailingEps_list, forwardEps_list, priceToBook_list, bookValue_list, industry_list = [], [], [], [], [], []

    # Iterate over unique tickers
    unique_tickers = stock_table['stock'].unique()
    for ticker in unique_tickers:
        url = f"https://brapi.dev/api/quote/{ticker}?modules=defaultKeyStatistics,summaryProfile&token=fUNoicPSZTzstUdqw1uVeB"
        response = requests.get(url)

        if response.status_code == 200:
            data = response.json()
            results = data.get('results', [{}])[0]

            # Extract and append the data
            priceEarnings_list.append(results.get('priceEarnings'))
            dks = results.get('defaultKeyStatistics', {})
            trailingEps_list.append(dks.get('trailingEps'))
            forwardEps_list.append(dks.get('forwardEps'))
            priceToBook_list.append(dks.get('priceToBook'))
            bookValue_list.append(dks.get('bookValue'))

            # Extract and append the industry data
            industry = results.get('summaryProfile', {}).get('industry')
            industry_list.append(industry)
        else:
            # Append None for missing data
            priceEarnings_list.append(None)
            trailingEps_list.append(None)
            forwardEps_list.append(None)
            priceToBook_list.append(None)
            bookValue_list.append(None)
            industry_list.append(None)

    # Add the data to the DataFrame
    stock_table['priceEarnings'] = priceEarnings_list
    stock_table['trailingEps'] = trailingEps_list
    stock_table['forwardEps'] = forwardEps_list
    stock_table['priceToBook'] = priceToBook_list
    stock_table['bookValue'] = bookValue_list
    stock_table['industry'] = industry_list

    return stock_table

# Usage example:
if __name__ == "__main__":
    stocks_df = import_API_data()
    if stocks_df is not None:
        print(stocks_df)


def import_B3_ClassifSetorial():
    # Specify the ZIP file URL
    zip_file_url = 'https://www.b3.com.br/data/files/57/E6/AA/A1/68C7781064456178AC094EA8/ClassifSetorial.zip'

    # Specify the destination file path
    zip_file_path = './ClassifSetorial.zip'

    # Check if the ZIP file already exists
    if os.path.exists(zip_file_path):
        print("ZIP file already exists. Overwriting...")

    # Download the ZIP file
    response = requests.get(zip_file_url)
    if response.status_code == 200:
        # Save the ZIP file to the specified path, overwriting if it exists
        with open(zip_file_path, 'wb') as file:
            file.write(response.content)
        print("ZIP file downloaded successfully:", zip_file_path)
    else:
        print("Failed to download the ZIP file.")
        return None  # Return None in case of failure

    # Unzip the ZIP file
    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
        zip_ref.extractall('./extracted_data/')

    # Find the extracted Excel file
    extracted_files = os.listdir('./extracted_data/')
    if len(extracted_files) == 1 and extracted_files[0].endswith('.xlsx'):
        excel_file_path = os.path.join('./extracted_data/', extracted_files[0])

        # Load the Excel file into a DataFrame
        df = pd.read_excel(excel_file_path, header=None, skiprows=8)  # Skip the first 8 rows

        # Extract 5-character strings starting from line 1 (row 0) and column D (4th column)
        codes = df.iloc[1:, 3].str.extract(r'(\w{4})')

        # Filter out empty cells and merged cells
        valid_codes = codes[codes[0].notna() & (codes[0].str.len() == 4)]

        # List the unique valid codes in the order they are read
        unique_valid_codes = valid_codes[0].tolist()
        unique_valid_codes = [code for code in unique_valid_codes if code not in ['LIST', 'DIGO', 'CÃ“DI']]

        print("File Name:", extracted_files[0])
        print("ZIP file unzipped successfully.")
    else:
        print("No valid Excel file found in the extracted directory.")

    # Initialize an empty list for tuples
    result_tuples = []

    # Index to keep track of position in unique_valid_codes
    index = 0

    # Variable to store the current value from Column C
    current_value_c = None

    # Variable to store the current value from Column B
    current_value_b = None

    # Variable to store the current value from Column A
    current_value_a = None

    # Iterate over each row in the DataFrame
    for idx, row in df.iterrows():
        # Check if Column D is empty
        if pd.isna(row[3]):
            # Check if Column C is not empty
            if not pd.isna(row[2]):
                # Retrieve the value from Column C and assign it to current_value_c
                current_value_c = row[2]

                # Check if Column B is not empty
                if not pd.isna(row[1]):
                    # Retrieve the value from Column B and assign it to current_value_b
                    current_value_b = row[1]

                # Check if Column A is not empty
                if not pd.isna(row[0]):
                    # Retrieve the value from Column A and assign it to current_value_a
                    current_value_a = row[0]

            else:
                # Erase the stored values when both columns C and D are empty
                current_value_c = None
                current_value_b = None
                current_value_a = None
            continue  # Skip to the next iteration without appending

        # Append the current values to a tuple with the corresponding code
        if current_value_c is not None and index < len(unique_valid_codes):
            result_tuples.append((unique_valid_codes[index], current_value_a, current_value_b, current_value_c))
            index += 1

        # If both conditions are met in the current row:
        # Condition 1: Column C is not empty.
        # Condition 2: Column D is empty.
        if not pd.isna(row[2]) and pd.isna(row[3]):
            if current_value_c is not None:
                result_tuples.append((unique_valid_codes[index], current_value_a, current_value_b, current_value_c))
                index += 1

        # Break the loop if we've exhausted unique_valid_codes
        if index >= len(unique_valid_codes):
            break

    # Create a DataFrame
    df = pd.DataFrame(result_tuples, columns=["TICKER", "ECONOMIC SECTOR", "SUBSECTOR", "SEGMENT"])

    # Return the final list of tuples
    return df

# Usage example:
if __name__ == "__main__":
    result_data = import_B3_ClassifSetorial()
    if result_data is not None:
        for t in result_data:
            print(t)


def merge_API_B3(stock_table, df):
    # Define a function to extract the first four characters
    def get_first_four_chars(code):
        return code[:4]

    # Apply this function to the 'stock' column in the stock_table DataFrame
    stock_table['short_ticker'] = stock_table['stock'].apply(get_first_four_chars)

    # Merge the columns 'ECONOMIC SECTOR', 'SUBSECTOR', 'SEGMENT' from df into stock_table
    stock_table = pd.merge(stock_table, df[['TICKER', 'ECONOMIC SECTOR', 'SUBSECTOR', 'SEGMENT']],
                          left_on='short_ticker', right_on='TICKER', how='left')

    # Drop the 'short_ticker' and 'TICKER' columns if no longer needed
    stock_table.drop(['short_ticker', 'TICKER'], axis=1, inplace=True)

    return stock_table

def filter_columns(df):
    # Retrieve unique values for each column
    unique_sectors = sorted(df['ECONOMIC SECTOR'].dropna().unique().tolist())
    unique_subsectors = sorted(df['SUBSECTOR'].dropna().unique().tolist())
    unique_segments = sorted(df['SEGMENT'].dropna().unique().tolist())

    # Return the lists
    return unique_sectors, unique_subsectors, unique_segments

def exclude_values_from_df(df, exclude_sectors=None, exclude_subsectors=None, exclude_segments=None):
    """
    Exclude specified values from the given DataFrame.

    Parameters:
    df (DataFrame): The DataFrame to be filtered.
    exclude_sectors (list): List of values to exclude from 'ECONOMIC SECTOR'.
    exclude_subsectors (list): List of values to exclude from 'SUBSECTOR'.
    exclude_segments (list): List of values to exclude from 'SEGMENT'.

    Returns:
    DataFrame: Filtered DataFrame.
    """
    filtered_df = df
    if exclude_sectors:
        filtered_df = filtered_df[~filtered_df['ECONOMIC SECTOR'].isin(exclude_sectors)]
    if exclude_subsectors:
        filtered_df = filtered_df[~filtered_df['SUBSECTOR'].isin(exclude_subsectors)]
    if exclude_segments:
        filtered_df = filtered_df[~filtered_df['SEGMENT'].isin(exclude_segments)]

    return filtered_df

def weighted_average_Economic_Sector(stock_table):
    # Define the fixed columns for which you want to calculate weighted averages
    columns_to_weight = ['change', 'priceEarnings', 'close', 'volume', 'trailingEps', 'forwardEps', 'priceToBook', 'bookValue']

    # Initialize an empty DataFrame to store the weighted averages
    weighted_avg_df = pd.DataFrame()

    # Initialize a flag to keep track of whether 'market_cap_sum' has been added
    market_cap_sum_added = False

    # Iterate over each column and calculate the weighted average
    for column_name in columns_to_weight:
        # Filter out rows with non-negative values for the current column, except for "change"
        if column_name != 'change':
            filtered_df = stock_table[(stock_table[column_name].notna()) & (stock_table[column_name] >= 0)]
        else:
            # Allow negative values in the "change" column
            filtered_df = stock_table[stock_table[column_name].notna()]

        # Group by 'ECONOMIC SECTOR'
        grouped = filtered_df.groupby('ECONOMIC SECTOR')

        # Calculate the weighted average for the current column
        weighted_avg = grouped.apply(lambda x: (x[column_name] * x['market_cap']).sum() / x['market_cap'].sum())

        # Rename the series to the column name with "avg"
        weighted_avg.name = column_name + '_avg'

        # Append the weighted average to the result DataFrame
        weighted_avg_df = pd.concat([weighted_avg_df, weighted_avg], axis=1)

        # If 'market_cap_sum' hasn't been added yet, add it
        if not market_cap_sum_added:
            # Calculate the sum of 'market_cap' for each sector
            sum_market_cap = grouped['market_cap'].sum()
            sum_market_cap.name = 'market_cap_sum'
            weighted_avg_df = pd.concat([weighted_avg_df, sum_market_cap], axis=1)
            market_cap_sum_added = True

    return weighted_avg_df





import pandas as pd

def weighted_average_Subsector(stock_table):
    # Define the fixed columns for which you want to calculate weighted averages
    columns_to_weight = ['change', 'priceEarnings', 'close', 'volume', 'trailingEps', 'forwardEps', 'priceToBook', 'bookValue']

    # Initialize an empty DataFrame to store the weighted averages
    weighted_avg_df = pd.DataFrame()

    # Initialize a flag to keep track of whether 'market_cap_sum' has been added
    market_cap_sum_added = False

    # Iterate over each column and calculate the weighted average
    for column_name in columns_to_weight:
        # Filter out rows with non-negative values for the current column, except for "change"
        if column_name != 'change':
            filtered_df = stock_table[(stock_table[column_name].notna()) & (stock_table[column_name] >= 0)]
        else:
            # Allow negative values in the "change" column
            filtered_df = stock_table[stock_table[column_name].notna()]

        # Group by 'SUBSECTOR'
        grouped = filtered_df.groupby('SUBSECTOR')

        # Calculate the weighted average for the current column
        weighted_avg = grouped.apply(lambda x: (x[column_name] * x['market_cap']).sum() / x['market_cap'].sum())

        # Rename the series to the column name with "avg"
        weighted_avg.name = column_name + '_avg'

        # Append the weighted average to the result DataFrame
        weighted_avg_df = pd.concat([weighted_avg_df, weighted_avg], axis=1)

        # If 'market_cap_sum' hasn't been added yet, add it
        if not market_cap_sum_added:
            # Calculate the sum of 'market_cap' for each subsector
            sum_market_cap = grouped['market_cap'].sum()
            sum_market_cap.name = 'market_cap_sum'
            weighted_avg_df = pd.concat([weighted_avg_df, sum_market_cap], axis=1)
            market_cap_sum_added = True

    return weighted_avg_df



import pandas as pd

def weighted_average_Segment(stock_table):
    # Define the fixed columns for which you want to calculate weighted averages
    columns_to_weight = ['change', 'priceEarnings', 'close', 'volume', 'trailingEps', 'forwardEps', 'priceToBook', 'bookValue']

    # Initialize an empty DataFrame to store the weighted averages
    weighted_avg_df = pd.DataFrame()

    # Initialize a flag to keep track of whether 'market_cap_sum' has been added
    market_cap_sum_added = False

    # Iterate over each column and calculate the weighted average
    for column_name in columns_to_weight:
        # Filter out rows with non-negative values for the current column, except for "change"
        if column_name != 'change':
            filtered_df = stock_table[(stock_table[column_name].notna()) & (stock_table[column_name] >= 0)]
        else:
            # Allow negative values in the "change" column
            filtered_df = stock_table[stock_table[column_name].notna()]

        # Group by 'SEGMENT'
        grouped = filtered_df.groupby('SEGMENT')

        # Calculate the weighted average for the current column
        weighted_avg = grouped.apply(lambda x: (x[column_name] * x['market_cap']).sum() / x['market_cap'].sum())

        # Rename the series to the column name with "avg"
        weighted_avg.name = column_name + '_avg'

        # Append the weighted average to the result DataFrame
        weighted_avg_df = pd.concat([weighted_avg_df, weighted_avg], axis=1)

        # If 'market_cap_sum' hasn't been added yet, add it
        if not market_cap_sum_added:
            # Calculate the sum of 'market_cap' for each segment
            sum_market_cap = grouped['market_cap'].sum()
            sum_market_cap.name = 'market_cap_sum'
            weighted_avg_df = pd.concat([weighted_avg_df, sum_market_cap], axis=1)
            market_cap_sum_added = True

    return weighted_avg_df


def weighted_average_Sector(stock_table):
    # Define the fixed columns for which you want to calculate weighted averages
    columns_to_weight = ['change', 'priceEarnings', 'close', 'volume', 'trailingEps', 'forwardEps', 'priceToBook', 'bookValue']

    # Initialize an empty DataFrame to store the weighted averages
    weighted_avg_df = pd.DataFrame()

    # Initialize a flag to keep track of whether 'market_cap_sum' has been added
    market_cap_sum_added = False

    # Iterate over each column and calculate the weighted average
    for column_name in columns_to_weight:
        # Filter out rows with non-negative values for the current column, except for "change"
        if column_name != 'change':
            filtered_df = stock_table[(stock_table[column_name].notna()) & (stock_table[column_name] >= 0)]
        else:
            # Allow negative values in the "change" column
            filtered_df = stock_table[stock_table[column_name].notna()]

        # Group by 'SECTOR'
        grouped = filtered_df.groupby('sector')

        # Calculate the weighted average for the current column
        weighted_avg = grouped.apply(lambda x: (x[column_name] * x['market_cap']).sum() / x['market_cap'].sum())

        # Rename the series to the column name with "avg"
        weighted_avg.name = column_name + '_avg'

        # Append the weighted average to the result DataFrame
        weighted_avg_df = pd.concat([weighted_avg_df, weighted_avg], axis=1)

        # If 'market_cap_sum' hasn't been added yet, add it
        if not market_cap_sum_added:
            # Calculate the sum of 'market_cap' for each sector
            sum_market_cap = grouped['market_cap'].sum()
            sum_market_cap.name = 'market_cap_sum'
            weighted_avg_df = pd.concat([weighted_avg_df, sum_market_cap], axis=1)
            market_cap_sum_added = True

    return weighted_avg_df


import pandas as pd

def weighted_average_Industry(stock_table):
    # Define the fixed columns for which you want to calculate weighted averages
    columns_to_weight = ['change', 'priceEarnings', 'close', 'volume', 'trailingEps', 'forwardEps', 'priceToBook', 'bookValue']

    # Initialize an empty DataFrame to store the weighted averages
    weighted_avg_df = pd.DataFrame()

    # Initialize a flag to keep track of whether 'market_cap_sum' has been added
    market_cap_sum_added = False

    # Iterate over each column and calculate the weighted average
    for column_name in columns_to_weight:
        # Filter out rows with non-negative values for the current column, except for "change"
        if column_name != 'change':
            filtered_df = stock_table[(stock_table[column_name].notna()) & (stock_table[column_name] >= 0)]
        else:
            # Allow negative values in the "change" column
            filtered_df = stock_table[stock_table[column_name].notna()]

        # Group by 'INDUSTRY'
        grouped = filtered_df.groupby('industry')

        # Calculate the weighted average for the current column
        weighted_avg = grouped.apply(lambda x: (x[column_name] * x['market_cap']).sum() / x['market_cap'].sum())

        # Rename the series to the column name with "avg"
        weighted_avg.name = column_name + '_avg'

        # Append the weighted average to the result DataFrame
        weighted_avg_df = pd.concat([weighted_avg_df, weighted_avg], axis=1)

        # If 'market_cap_sum' hasn't been added yet, add it
        if not market_cap_sum_added:
            # Calculate the sum of 'market_cap' for each industry
            sum_market_cap = grouped['market_cap'].sum()
            sum_market_cap.name = 'market_cap_sum'
            weighted_avg_df = pd.concat([weighted_avg_df, sum_market_cap], axis=1)
            market_cap_sum_added = True

    return weighted_avg_df


import pandas as pd

def weighted_average_Bovespa(stock_table):
    # Define the columns you want to calculate weighted averages for
    columns_to_weight = ['priceEarnings', 'close', 'trailingEps', 'forwardEps', 'priceToBook', 'bookValue']

    # Initialize an empty DataFrame to store the weighted averages
    weighted_avg_df = pd.DataFrame()

    # Iterate over each column and calculate the weighted average
    for column_name in columns_to_weight:
        # Filter out rows with non-negative values for the current column
        filtered_df = stock_table[(stock_table[column_name].notna()) & (stock_table[column_name] >= 0)]

        # Calculate the weighted average for the current column
        weighted_avg = (filtered_df[column_name] * filtered_df['market_cap']).sum() / filtered_df['market_cap'].sum()

        # Check if the weighted_avg is NaN (no valid data)
        if not pd.notna(weighted_avg):
            weighted_avg = None

        # Rename the series to the column name
        weighted_avg_df[column_name] = [weighted_avg]

    # Calculate the sum of 'market_cap'
    market_cap_sum = stock_table['market_cap'].sum()

    # Add 'market_cap_sum' to the result DataFrame
    weighted_avg_df['market_cap_sum'] = [market_cap_sum]

    return weighted_avg_df


def calculate_fair_price_in_out(stock_table, stock_average_economic_sector, stock_average_subsector, stock_average_segment):
    # Create 'fair_price_in_out' DataFrame
    fair_price_in_out = stock_table[['stock', 'priceEarnings', 'trailingEps', 'priceToBook', 'bookValue']].copy()

    # Calculate the square root of the product for each row if none are NaN or negative
    fair_price_in_out['fair_price_stock'] = fair_price_in_out.apply(
        lambda row: np.sqrt(row['priceEarnings'] * row['trailingEps'] * row['priceToBook'] * row['bookValue'])
        if all(row[x] >= 0 and not pd.isna(row[x]) for x in ['priceEarnings', 'trailingEps', 'priceToBook', 'bookValue'])
        else None, axis=1)

    # Copy and rename the 'market_cap' column from 'stock_table'
    fair_price_in_out['market_cap_stock'] = stock_table['market_cap']

    # Add 'ECONOMIC SECTOR', 'SUBSECTOR', and 'SEGMENT' columns from 'stock_table'
    fair_price_in_out = pd.merge(fair_price_in_out, stock_table[['stock', 'ECONOMIC SECTOR', 'SUBSECTOR', 'SEGMENT']], on='stock')

    # Apply the retrieval logic to each row in 'fair_price_in_out' for ECONOMIC SECTOR using a lambda function
    fair_price_in_out['priceEarnings_econ_sec'] = fair_price_in_out.apply(lambda row: 
        stock_average_economic_sector.loc[row['ECONOMIC SECTOR'], 'priceEarnings_avg'] if row['ECONOMIC SECTOR'] in stock_average_economic_sector.index else None, axis=1)

    fair_price_in_out['market_cap_econ_sec'] = fair_price_in_out.apply(lambda row: 
        stock_average_economic_sector.loc[row['ECONOMIC SECTOR'], 'market_cap_sum'] if row['ECONOMIC SECTOR'] in stock_average_economic_sector.index else None, axis=1)

    # Apply the retrieval logic to each row in 'fair_price_in_out' for SUBSECTOR using a lambda function
    fair_price_in_out['priceEarnings_subsector'] = fair_price_in_out.apply(lambda row: 
        stock_average_subsector.loc[row['SUBSECTOR'], 'priceEarnings_avg'] if row['SUBSECTOR'] in stock_average_subsector.index else None, axis=1)

    fair_price_in_out['market_cap_subsector'] = fair_price_in_out.apply(lambda row: 
        stock_average_subsector.loc[row['SUBSECTOR'], 'market_cap_sum'] if row['SUBSECTOR'] in stock_average_subsector.index else None, axis=1)

    # Apply the retrieval logic to each row in 'fair_price_in_out' for SEGMENT using a lambda function
    fair_price_in_out['priceEarnings_segment'] = fair_price_in_out.apply(lambda row: 
        stock_average_segment.loc[row['SEGMENT'], 'priceEarnings_avg'] if row['SEGMENT'] in stock_average_segment.index else None, axis=1)

    fair_price_in_out['market_cap_segment'] = fair_price_in_out.apply(lambda row: 
        stock_average_segment.loc[row['SEGMENT'], 'market_cap_sum'] if row['SEGMENT'] in stock_average_segment.index else None, axis=1)

    # Calculate the weighted mean for each row
    fair_price_in_out['fair_price_in_out_trailing'] = fair_price_in_out.apply(lambda row: 
        np.average(
            [row['fair_price_stock'], row['priceEarnings_econ_sec'], row['priceEarnings_subsector'], row['priceEarnings_segment']],
            weights=[row['market_cap_stock'], row['market_cap_econ_sec'], row['market_cap_subsector'], row['market_cap_segment']]
        ) if not pd.isna(row['fair_price_stock']) else None, axis=1)

    # Create the 'fair_price_in_out_trailing' DataFrame with 'stock' and 'fair_price_in_out_trailing' columns
    fair_price_in_out_trailing = fair_price_in_out[['stock', 'fair_price_in_out_trailing']]
    
    return fair_price_in_out_trailing




